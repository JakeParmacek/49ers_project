# Project Proposal: The "Shanahan Algorithm"
### Predicting San Francisco 49ers Play-Calling Strategy using Machine Learning

## 1. Problem Statement
In the NFL, anticipating the opponent's next move is the key to defensive success. While coaches rely on intuition and film study, play-calling often follows distinct statistical patterns based on game situations.

**The Objective:** This project aims to build a **Binary Classification Model** to predict whether the San Francisco 49ers will execute a **Run** or a **Pass** play. By analyzing the "Kyle Shanahan Era" (2017–Present), we aim to determine if an AI model can identify play-calling tendencies more accurately than a naive baseline.

## 2. Data Curation & Engineering
Unlike standard pre-packaged datasets, this project will utilize a manually curated dataset built from scratch to ensure data integrity and relevance.

* **Data Source:** Raw play-by-play logs scraped from **Pro-Football-Reference (PFR)**.
* **Scope:** The "Shanahan Era" (2017 – Present).
* **Target Variable (Y):** `Play Type` (0 = Run, 1 = Pass).
* **Data Cleaning Pipeline:**
    * Scraping raw HTML tables using Python (`pandas`, `requests`).
    * Filtering out non-offensive plays (Punts, Field Goals, Kneel-downs, Timeouts).
    * Parsing text descriptions (e.g., converting "1st & 10 at SFO 25" into integer features).

### Key Features (X Variables)
The model will be trained on the following state-based features:
1.  **Down:** (1, 2, 3, 4)
2.  **Distance:** Yards to gain for a first down.
3.  **Field Position:** Distance from the end zone (1-99).
4.  **Score Differential:** (49ers Score - Opponent Score).
5.  **Time Remaining:** Seconds remaining in the half/game.
6.  **Formation/Personnel:** (Derived from text, e.g., "Shotgun", "No Huddle").

## 3. Methodology

### A. Data Splitting Strategy
To prevent **Data Leakage** (predicting the past using the future), we will use a strict **Chronological Split** rather than a random shuffle.

| Set | Season Years | Purpose |
| :--- | :--- | :--- |
| **Training** | 2017 – 2023 | The model learns the historical patterns of Shanahan's system during his peak success years. |
| **Validation** | 2024 | Used for Hyperparameter Tuning. *Note: The 49ers went 6-11 this season, providing a crucial "stress test" to see if the model works when the team is struggling.* |
| **Testing** | 2025 (Current) | The final "blind" evaluation using live data from the current ongoing season (Weeks 1–Present). |

### B. Models for Comparison
We will train and compare three distinct approaches to measure performance gains:

1.  **Logistic Regression (Baseline):**
    * *Why:* Provides a simple, interpretable probabilistic baseline. It establishes the "floor" for performance and reveals feature importance (e.g., "How much does being on 3rd Down increase pass probability?").
2.  **Random Forest Classifier:**
    * *Why:* Capable of capturing non-linear interactions (e.g., "Shanahan usually passes on 2nd down, *unless* they are winning by 10 points, *and* it's the 4th quarter").
3.  **XGBoost (Extreme Gradient Boosting):**
    * *Why:* The industry standard for tabular data. It minimizes bias and variance by sequentially training trees to correct previous errors.

## 4. Evaluation Metrics
Since accuracy alone can be misleading in sports (e.g., if a team passes 60% of the time, guessing "Pass" every time yields 60% accuracy), we will use the following metrics:

* **Accuracy:** Overall percentage of correct predictions.
* **Confusion Matrix:** To visualize the balance between False Positives (Predicting Pass when it was a Run) and False Negatives.
* **Precision vs. Recall:** specifically for the "Run" class, as predicting runs is often more difficult than predicting passes in the modern NFL.

## 5. Hypothesis & Expected Outcome
* **Hypothesis:** We hypothesize that `Distance` and `Score Differential` will be the strongest predictors, but that `Time Remaining` will have a non-linear impact (e.g., only matters in the last 2 minutes of halves).
* **Success Metric:** A successful model should achieve an accuracy on the Test Set (2025 data) that exceeds **68%**, significantly outperforming a random guess (50%) or a majority-class baseline (~58%).

## 6. Tools & Technologies
* **Language:** Python 3.9+
* **Libraries:** `Pandas` (Data Manipulation), `BeautifulSoup` (Scraping), `Scikit-Learn` (Modeling), `XGBoost` (Advanced Modeling), `Matplotlib/Seaborn` (Visualization).